{
  "llm_config": {
  "backupModel": "xxxxx",
	"model": "gpt-4o", 
	"temperature": 0.2,
  "max_tokens": 5000,
  "timeout": 10
  },
  "prompts":{
    "generic": "generic_prompt.md",
    "prompts_path": "prompts"
  }
}
